{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    file=open(filename,mode='rt',encoding='utf-8')\n",
    "    text=file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "def to_sentences(doc):\n",
    "    return doc.strip().split('\\n')\n",
    "def sentence_lengths(sentences):\n",
    "    lengths=[len(s.split()) for s in sentences]\n",
    "    return min(lengths),max(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english data: sentences=2007723, min=0, max=668\n",
      "french data: sentences=2007723, min=0, max=693\n"
     ]
    }
   ],
   "source": [
    "filename='europarl-v7.fr-en.en'\n",
    "doc=load_doc(filename)\n",
    "sentences=to_sentences(doc)\n",
    "\n",
    "minlen,maxlen=sentence_lengths(sentences)\n",
    "print('english data: sentences=%d, min=%d, max=%d'%(len(sentences),minlen,maxlen))\n",
    "\n",
    "\n",
    "filename='europarl-v7.fr-en.fr'\n",
    "doc=load_doc(filename)\n",
    "sentences=to_sentences(doc)\n",
    "\n",
    "minlen,maxlen=sentence_lengths(sentences)\n",
    "print('french data: sentences=%d, min=%d, max=%d'%(len(sentences),minlen,maxlen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEAN DATASET"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Tokenizing text by white space.\n",
    "Normalizing case to lowercase.\n",
    "Removing punctuation from each word.\n",
    "Removing non-printable characters.\n",
    "Converting French characters to Latin characters.\n",
    "Removing words that contain non-alphabetic characters."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# cleaning\n",
    "#1.lower case\n",
    "def lower_case(data):\n",
    "    return data.lower()\n",
    "\n",
    "#2.tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "def tokenize_words(data):\n",
    "    return word_tokenize(data)\n",
    "from nltk.tokenize import sent_tokenize\n",
    "def tokenize_sentence(data):\n",
    "    return sent_tokenize(data)\n",
    "    \n",
    "#3.punctuation\n",
    "import string\n",
    "def punct(data):\n",
    "    obj=str.maketrans('','',string .punctuation)\n",
    "    ans=data.translate(obj)\n",
    "    return ans\n",
    "\n",
    "#4.contraction\n",
    "def contract(data):\n",
    "    tokens=word_tokenize(data)\n",
    "    req=[]\n",
    "    for word in tokens:\n",
    "        if word in appos:\n",
    "            req.append(appos[word])\n",
    "        else :\n",
    "            rep.append(word)\n",
    "    ans=' '.join(req)\n",
    "    return ans\n",
    "\n",
    "#5.stop words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_word=set(stopwords.words(\"english\"))\n",
    "def stop_words(data):\n",
    "    tokens=word_tokenize(data)\n",
    "    req=[]\n",
    "    for words in tokens:\n",
    "        if words not in stop_word:\n",
    "            req.append(words)\n",
    "    ans=' '.join(req)\n",
    "    return ans\n",
    "\n",
    "#6.apha numeric words\n",
    "import re\n",
    "def alpha_numeric_words(data):\n",
    "    pattern= r'[^a-zA-z\\s]'\n",
    "    text=re.sub(pattern,'',data)\n",
    "    return text\n",
    "\n",
    "#7.morphological normaization\n",
    "\n",
    "#a.stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "def stemming(data):\n",
    "    stemmer=PorterStemmer()\n",
    "    req=[]\n",
    "    for word in data.split():\n",
    "        req.append(stemmer.stem(word))\n",
    "    ans=' '.join(req)\n",
    "    return ans    \n",
    "#b.lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def lemmatization(data):\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    req=[]\n",
    "    for words in data.split():\n",
    "        req.append(lemmatizer.lemmatize(words))\n",
    "    ans=' '.join(req)\n",
    "    return ans\n",
    "\n",
    "data=\"This is$ one of the @ most ones of oned way!\"\n",
    "print(lower_case(data))\n",
    "print(tokenize_words(data))\n",
    "print(tokenize_sentence(data))\n",
    "print(punct(data))\n",
    "#print(contract(data)) due to appos is not working\n",
    "print(stop_words(data))\n",
    "print(alpha_numeric_words(data))\n",
    "print(stemming(data))\n",
    "print(lemmatization(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_lines(lines):\n",
    "    cleaned=list()\n",
    "    re_print=re.compile('[^%s]' % re.escape(string.printable))#prepare regex for char filtering\n",
    "    table=str.maketrans('','',string.punctuation)#prepare translation table for removing punctuation\n",
    "    for lin in lines:\n",
    "        line=normalize('NFD',line).encode('ascii','ignore')\n",
    "        line=line.decode('UTF-8')\n",
    "        lin=line.split()\n",
    "        line=[word.lower() for word in line]\n",
    "        line=[word.translate(table) for word in line]#remove punctuation from each token\n",
    "        line=[re_print.sub('',w) for w in line]#remove non-printable characters from each token\n",
    "        line=[word for word in line if word.isalpha()]#remove tokens with number in them\n",
    "        cleaned.append(' '.join(line))\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english.pkl\n",
      "Resumption of the session\n",
      "I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n",
      "Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n",
      "You have requested a debate on this subject in the course of the next few days, during this part-session.\n",
      "In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\n",
      "Please rise, then, for this minute' s silence.\n",
      "(The House rose and observed a minute' s silence)\n",
      "Madam President, on a point of order.\n",
      "You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.\n",
      "One of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago.\n",
      "\n",
      "\n",
      "Saved: french.pkl\n",
      "Reprise de la session\n",
      "Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances.\n",
      "Comme vous avez pu le constater, le grand \"bogue de l'an 2000\" ne s'est pas produit. En revanche, les citoyens d'un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles.\n",
      "Vous avez souhaité un débat à ce sujet dans les prochains jours, au cours de cette période de session.\n",
      "En attendant, je souhaiterais, comme un certain nombre de collègues me l'ont demandé, que nous observions une minute de silence pour toutes les victimes, des tempêtes notamment, dans les différents pays de l'Union européenne qui ont été touchés.\n",
      "Je vous invite à vous lever pour cette minute de silence.\n",
      "(Le Parlement, debout, observe une minute de silence)\n",
      "Madame la Présidente, c'est une motion de procédure.\n",
      "Vous avez probablement appris par la presse et par la télévision que plusieurs attentats à la bombe et crimes ont été perpétrés au Sri Lanka.\n",
      "L'une des personnes qui vient d'être assassinée au Sri Lanka est M. Kumar Ponnambalam, qui avait rendu visite au Parlement européen il y a quelques mois à peine.\n"
     ]
    }
   ],
   "source": [
    "#now we will save the lists of clean lines directly in binary format using pickle\n",
    "import string \n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "\n",
    "def save_clean_sentences(sentences,filename):\n",
    "    dump(sentences,open(filename,'wb'))\n",
    "    print('Saved: %s'%filename)\n",
    "    \n",
    "filename='europarl-v7.fr-en.en'\n",
    "doc=load_doc(filename)\n",
    "sentences=to_sentences(doc)\n",
    "save_clean_sentences(sentences,'english.pkl')\n",
    "for i in range(10):\n",
    "    print(sentences[i])\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "filename='europarl-v7.fr-en.fr'\n",
    "doc=load_doc(filename)\n",
    "sentences=to_sentences(doc)\n",
    "save_clean_sentences(sentences,'french.pkl')\n",
    "for i in range(10):\n",
    "    print(sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REDUCE VOCABULARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary: 301951\n",
      "New English Vocabulary: 86944\n",
      "Saved: english_vocab.pkl\n",
      "Resumption of the session\n",
      "I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n",
      "Although, as you will have seen, the dreaded unk unk failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n",
      "You have requested a debate on this subject in the course of the next few days, during this part-session.\n",
      "In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\n",
      "Please rise, then, for this minute' s silence.\n",
      "(The House rose and observed a minute' s silence)\n",
      "Madam President, on a point of order.\n",
      "You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.\n",
      "One of the people assassinated very recently in Sri Lanka was Mr unk unk who had visited the European Parliament just a few months ago.\n",
      "French Vocabulary: 395651\n",
      "New French Vocabulary: 117594\n",
      "Saved: french_vocab.pkl\n",
      "Reprise de la session\n",
      "Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances.\n",
      "Comme vous avez pu le constater, le grand unk de l'an 2000\" ne s'est pas produit. En revanche, les citoyens d'un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles.\n",
      "Vous avez souhaité un débat à ce sujet dans les prochains jours, au cours de cette période de session.\n",
      "En attendant, je souhaiterais, comme un certain nombre de collègues me l'ont demandé, que nous observions une minute de silence pour toutes les victimes, des tempêtes notamment, dans les différents pays de l'Union européenne qui ont été touchés.\n",
      "Je vous invite à vous lever pour cette minute de silence.\n",
      "(Le Parlement, debout, observe une minute de silence)\n",
      "Madame la Présidente, c'est une motion de procédure.\n",
      "Vous avez probablement appris par la presse et par la télévision que plusieurs attentats à la bombe et crimes ont été perpétrés au Sri Lanka.\n",
      "L'une des personnes qui vient d'être assassinée au Sri Lanka est M. unk unk qui avait rendu visite au Parlement européen il y a quelques mois à peine.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pickle import load\n",
    "from pickle import dump\n",
    "from collections import Counter\n",
    "\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "# save a list of clean sentences to file\n",
    "def save_clean_sentences(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    "\n",
    "# create a frequency table for all words\n",
    "def to_vocab(lines):\n",
    "    vocab = Counter()\n",
    "    for line in lines:\n",
    "        tokens = line.split()\n",
    "        vocab.update(tokens)\n",
    "    return vocab\n",
    "\n",
    "# remove all words with a frequency below a threshold\n",
    "def trim_vocab(vocab, min_occurance):\n",
    "    tokens = [k for k,c in vocab.items() if c >= min_occurance]\n",
    "    return set(tokens)\n",
    "\n",
    "# mark all OOV with \"unk\" for all lines\n",
    "def update_dataset(lines, vocab):\n",
    "    new_lines = list()\n",
    "    for line in lines:\n",
    "        new_tokens = list()\n",
    "        for token in line.split():\n",
    "            if token in vocab:\n",
    "                new_tokens.append(token)\n",
    "            else:\n",
    "                new_tokens.append('unk')\n",
    "        new_line = ' '.join(new_tokens)\n",
    "        new_lines.append(new_line)\n",
    "    return new_lines\n",
    "\n",
    "# load English dataset\n",
    "filename = 'english.pkl'\n",
    "lines = load_clean_sentences(filename)\n",
    "# calculate vocabulary\n",
    "vocab = to_vocab(lines)\n",
    "print('English Vocabulary: %d' % len(vocab))\n",
    "# reduce vocabulary\n",
    "vocab = trim_vocab(vocab, 5)\n",
    "print('New English Vocabulary: %d' % len(vocab))\n",
    "# mark out of vocabulary words\n",
    "lines = update_dataset(lines, vocab)\n",
    "# save updated dataset\n",
    "filename = 'english_vocab.pkl'\n",
    "save_clean_sentences(lines, filename)\n",
    "# spot check\n",
    "for i in range(10):\n",
    "    print(lines[i])\n",
    "\n",
    "# load French dataset\n",
    "filename = 'french.pkl'\n",
    "lines = load_clean_sentences(filename)\n",
    "# calculate vocabulary\n",
    "vocab = to_vocab(lines)\n",
    "print('French Vocabulary: %d' % len(vocab))\n",
    "# reduce vocabulary\n",
    "vocab = trim_vocab(vocab, 5)\n",
    "print('New French Vocabulary: %d' % len(vocab))\n",
    "# mark out of vocabulary words\n",
    "lines = update_dataset(lines, vocab)\n",
    "# save updated dataset\n",
    "filename = 'french_vocab.pkl'\n",
    "save_clean_sentences(lines, filename)\n",
    "# spot check\n",
    "for i in range(10):\n",
    "    print(lines[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
